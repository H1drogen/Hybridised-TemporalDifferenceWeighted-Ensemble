{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "env_name = \"CartPole-v0\"\n",
    "# env_name = \"MountainCar-v0\"\n",
    "# env_name = \"LunarLander-v3\"\n",
    "seed = 2\n",
    "initial_training = 10\n",
    "max_evaluations = 100\n",
    "dqn_path = None\n",
    "model_dqn_path = None\n"
   ],
   "id": "9ec697ad5d8c4ce2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(q, temp=1.0):\n",
    "    q /= temp\n",
    "    return np.exp(q - np.max(q)) / np.sum(np.exp(q - np.max(q)))\n",
    "\n",
    "class AccumulateErrorEnsemble:\n",
    "    def __init__(self, models, gamma=0.99, decay=1.0, temp=1.0):\n",
    "        self.models = models\n",
    "        self.gamma = gamma\n",
    "        self.decay = decay\n",
    "        self.temp = temp\n",
    "        self.cumulative_errors = np.zeros(len(models))\n",
    "        self.prev_qs = None\n",
    "\n",
    "    def _update_prev_qs(self, obs):\n",
    "        if self.prev_qs is None:\n",
    "            self.prev_qs = []\n",
    "            for model in self.models:\n",
    "                self.prev_qs.append(model.infer(obs)[0])\n",
    "\n",
    "    def _get_weights(self):\n",
    "        return softmax(-self.cumulative_errors, self.temp)\n",
    "\n",
    "    def act(self, obs):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def observe(self, action, obs, reward, done):\n",
    "        if done:\n",
    "            self.cumulative_errors *= 0.0\n",
    "            self.prev_qs = None\n",
    "            return\n",
    "        self.cumulative_errors *= self.decay\n",
    "        next_qs = []\n",
    "        for i, model in enumerate(self.models):\n",
    "            next_q = model.infer(obs)[0]\n",
    "            next_qs.append(next_q)\n",
    "            q = self.prev_qs[i][action]\n",
    "            error = reward + self.gamma * np.max(next_q) - q\n",
    "            self.cumulative_errors[i] += error ** 2\n",
    "        self.prev_qs = next_qs\n",
    "\n",
    "\n",
    "class TDWAverageEnsemble(AccumulateErrorEnsemble):\n",
    "    def __init__(self, models, gamma=0.99, decay=1.0, temp=1.0, visualizer=None):\n",
    "        self.visualizer = visualizer\n",
    "        super().__init__(models, gamma, decay, temp)\n",
    "\n",
    "    def act(self, obs):\n",
    "        self._update_prev_qs(obs)\n",
    "\n",
    "        weights = self._get_weights()\n",
    "\n",
    "        if self.visualizer is not None:\n",
    "            self.visualizer.update(weights)\n",
    "\n",
    "        weighted_q = np.reshape(weights, [-1, 1]) * np.array(self.prev_qs)\n",
    "        q = np.sum(weighted_q, axis=0)\n",
    "        action = np.argmax(q)\n",
    "        self.prev_action = action\n",
    "        return action\n",
    "\n",
    "\n",
    "class TDWVoteEnsemble(AccumulateErrorEnsemble):\n",
    "    def __init__(self, models, gamma=0.99, decay=1.0, temp=1.0, visualizer=None):\n",
    "        self.visualizer = visualizer\n",
    "        super().__init__(models, gamma, decay, temp)\n",
    "\n",
    "    def act(self, obs):\n",
    "        self._update_prev_qs(obs)\n",
    "        # check self.prev_qs\n",
    "\n",
    "        weights = self._get_weights()\n",
    "        print(\"weights:\", weights)\n",
    "\n",
    "        if self.visualizer is not None:\n",
    "            self.visualizer.update(weights)\n",
    "\n",
    "        votes = np.zeros(len(self.prev_qs[0]), dtype=np.float32)\n",
    "        # for w, q in zip(weights, self.prev_qs):\n",
    "        #     votes[np.argmax(q)] += w\n",
    "\n",
    "        agent_contributions = np.zeros(len(self.prev_qs), dtype=np.float32)\n",
    "        for i, (w, q) in enumerate(zip(weights, self.prev_qs)):\n",
    "            action_index = np.argmax(q)\n",
    "            votes[action_index] += w\n",
    "            agent_contributions[i] += w if action_index == np.argmax(votes) else 0\n",
    "\n",
    "        action = np.argmax(votes)\n",
    "        most_contributing_agent = np.argmax(agent_contributions)\n",
    "\n",
    "        self.prev_action = action\n",
    "        return action, most_contributing_agent"
   ],
   "id": "7eff04c3678b61e0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self,env):\n",
    "        pass\n",
    "\n",
    "    def act(self, state):\n",
    "        pass\n",
    "\n",
    "    def update_model(self, state, action, reward, new_state, done):\n",
    "        pass\n",
    "\n",
    "class DQN_Agent(Agent):\n",
    "    def __init__(self, env, name=\"DQN_Agent\"):\n",
    "        self.name = name\n",
    "        self.env = env\n",
    "        self.replay_memory = deque(maxlen=200000)\n",
    "\n",
    "        self.gamma = 0.99\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.9995\n",
    "        self.learning_rate = 0.05\n",
    "        self.target_update_counter = 0\n",
    "        self.C = 8 # intervcal for updating target network\n",
    "        self.initial_random_steps = 0\n",
    "        self.actions_count = 0\n",
    "        self.clip_errors = True\n",
    "\n",
    "        self.q_network = self.init_q_network()\n",
    "        self.target_q_network = self.init_q_network()\n",
    "\n",
    "    def on_episode_start(self):\n",
    "        pass\n",
    "\n",
    "    def on_episode_end(self):\n",
    "        pass\n",
    "\n",
    "    def get_observation_space(self):\n",
    "        return self.env.observation_space\n",
    "\n",
    "    def get_action_space(self):\n",
    "        return self.env.action_space\n",
    "\n",
    "    def init_q_network(self):\n",
    "        model = Sequential()\n",
    "        state_shape = self.get_observation_space().shape\n",
    "        model.add(Dense(48, input_shape=state_shape, activation=\"relu\"))\n",
    "        #model.add(Dense(48, activation=\"relu\"))\n",
    "        #model.add(Dense(24, activation=\"relu\"))\n",
    "        model.add(Dense(self.get_action_space().n, activation='linear'))\n",
    "        model.compile(loss=\"mean_squared_error\", optimizer=Adam(learning_rate=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "\n",
    "\n",
    "    def act(self, state):\n",
    "        self.actions_count += 1\n",
    "        self.epsilon *= self.epsilon_decay\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon)\n",
    "        if np.random.random() < self.epsilon or self.actions_count < self.initial_random_steps:\n",
    "            return self.get_action_space().sample()\n",
    "        return np.argmax(self.q_network.predict(state)[0])\n",
    "\n",
    "    def infer(self, obs_t):\n",
    "        if isinstance(obs_t, np.ndarray) and obs_t.ndim > 3:\n",
    "            return self.q_network.predict(obs_t)\n",
    "        obs_t = np.array(obs_t).reshape(1, -1)\n",
    "        q_values = self.q_network.predict(obs_t) # Perform inference using the Q-network\n",
    "        return q_values\n",
    "\n",
    "\n",
    "    def update_model(self, state, action, reward, new_state, done):\n",
    "        self.replay_memory.append([state, action, reward, new_state, done])\n",
    "        self.fit_q_network()\n",
    "        self.update_target_q_network()\n",
    "\n",
    "    def sample_replays(self,batch_size):\n",
    "        return random.sample(self.replay_memory, batch_size)\n",
    "\n",
    "    def fit_q_network(self):\n",
    "        #sample replay and do SGD\n",
    "        batch_size = 16\n",
    "        if len(self.replay_memory) < batch_size:\n",
    "            return\n",
    "\n",
    "        samples = self.sample_replays(batch_size)\n",
    "        sampled_states = []\n",
    "        sampled_targets = []\n",
    "        for sample in samples:\n",
    "            state, action, reward, new_state, done = sample\n",
    "            target = self.target_q_network.predict(state)\n",
    "            predicted = self.q_network.predict(state)\n",
    "            if done:\n",
    "                target[0][action] = reward\n",
    "            else:\n",
    "                #update target by Bellman equation\n",
    "                target[0][action] = reward + self.gamma * max(self.target_q_network.predict(new_state)[0])\n",
    "\n",
    "                if self.clip_errors:\n",
    "                    #clip error to -1, +1\n",
    "                    if (target[0][action] > predicted[0][action]):\n",
    "                        target[0][action] = predicted[0][action] + 1\n",
    "                    elif (target[0][action] > predicted[0][action]):\n",
    "                        target[0][action] = predicted[0][action] - 1\n",
    "            sampled_states.append(state)\n",
    "            sampled_targets.append(target)\n",
    "\n",
    "        batched_states = np.concatenate(sampled_states,axis=0)\n",
    "        batched_targets = np.concatenate(sampled_targets,axis=0)\n",
    "        self.q_network.fit(batched_states, batched_targets, epochs=1, verbose=0)\n",
    "\n",
    "    def update_target_q_network(self):\n",
    "        #update target q network every C steps\n",
    "        self.target_update_counter += 1\n",
    "        if (self.target_update_counter > self.C):\n",
    "            self.target_update_counter = 0\n",
    "            self.target_q_network.set_weights(self.q_network.get_weights())"
   ],
   "id": "a0622529754e2a0a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from scipy import stats\n",
    "\n",
    "class DQN_Guided_Exploration(DQN_Agent):\n",
    "    def __init__(self, env, name=\"DQN_Guided_Exploration\", gamma=0.99, epsilon=1.0, epsilon_min=0.01, epsilon_decay=0.9995, learning_rate=0.05):\n",
    "        self.name = name\n",
    "        self.env = env\n",
    "        self.replay_memory = deque(maxlen=200000)\n",
    "\n",
    "        #Mountain Car\n",
    "        #explore sample = 50\n",
    "        #qnetwork = 1 hiddenlayer 48 units\n",
    "        #convergence cutoff 0.0003\n",
    "        #dynamics network lr = 0.02\n",
    "        #dynamics network batchsize =64\n",
    "        #scatter plot 2000 sample\n",
    "        self.gamma = 0.99\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.9995\n",
    "        self.learning_rate = 0.05\n",
    "        self.target_update_counter = 0\n",
    "        self.C = 8 # intervcal for updating target network\n",
    "        self.initial_random_steps = 0\n",
    "        self.actions_count = 0\n",
    "        self.clip_errors = True\n",
    "\n",
    "        '''#Lunar\n",
    "        self.gamma = 0.99\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.9995\n",
    "        self.learning_rate = 0.05\n",
    "        self.target_update_counter = 0\n",
    "        self.C = 8 # intervcal for updating target network\n",
    "        self.initial_random_steps = 5000\n",
    "        self.actions_count = 0\n",
    "        self.clip_errors = True'''\n",
    "\n",
    "        self.q_network = self.init_q_network()\n",
    "        self.target_q_network = self.init_q_network()\n",
    "        self.dynamics_model = self.init_dynamics_model()\n",
    "        self.update_count = 0\n",
    "        self.dynamics_model_converged = False\n",
    "\n",
    "    def update_model(self, state, action, reward, new_state, done):\n",
    "\n",
    "        self.replay_memory.append([state, action, reward, new_state, done])\n",
    "        self.fit_q_network()\n",
    "        self.update_target_q_network()\n",
    "        self.update_count += 1\n",
    "\n",
    "        if self.update_count % 25 == 0:\n",
    "            self.fit_dynamics_model()\n",
    "        if self.update_count % 500 == 0:\n",
    "            self.eval_dynamics_model()\n",
    "\n",
    "    def act(self, state):\n",
    "        self.actions_count += 1\n",
    "        self.epsilon *= self.epsilon_decay\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon)\n",
    "        if np.random.random() < self.epsilon or self.actions_count < self.initial_random_steps:\n",
    "            return self.explore(state)\n",
    "        return np.argmax(self.q_network.predict(state)[0])\n",
    "\n",
    "    def explore(self,state):\n",
    "        if not self.dynamics_model_converged:\n",
    "            return self.get_action_space().sample()\n",
    "        #return self.get_action_space().sample()\n",
    "        N = len(self.replay_memory)\n",
    "        num_samples = 50\n",
    "        samples = []\n",
    "        for i in range(N-num_samples,N):\n",
    "           samples.append(self.replay_memory[i][0])\n",
    "\n",
    "        least_p = np.inf\n",
    "        best_a = -1\n",
    "        for action in range(self.get_action_space().n):\n",
    "            next_state = self.dynamics_model.predict(np.append(state, [[action]], axis=1))\n",
    "            p = self.get_probability(next_state, samples)\n",
    "            if p < least_p:\n",
    "                best_a = action\n",
    "                least_p = p\n",
    "        return best_a\n",
    "\n",
    "    def get_probability(self,state, samples):\n",
    "        design = []\n",
    "        for s in samples:\n",
    "            design.append(s[0])\n",
    "        design = np.stack(design).T\n",
    "        cov = np.cov(design)\n",
    "        mean = np.mean(design,axis = 1)\n",
    "        p = stats.multivariate_normal.pdf(state[0],mean,cov)\n",
    "        return p\n",
    "\n",
    "    def init_dynamics_model(self):\n",
    "        model = Sequential()\n",
    "        state_shape = (self.get_observation_space().shape[0] + 1,)\n",
    "        print(state_shape)\n",
    "        model.add(Dense(24, input_shape=state_shape, activation=\"relu\"))\n",
    "        model.add(Dense(24, activation=\"relu\"))\n",
    "        model.add(Dense(self.get_observation_space().shape[0], activation='linear'))\n",
    "        model.compile(loss=\"mean_squared_error\", optimizer=Adam(learning_rate=0.02))\n",
    "        return model\n",
    "\n",
    "    def fit_dynamics_model(self):\n",
    "        batchsize = 64\n",
    "        if len(self.replay_memory) < batchsize:\n",
    "            return\n",
    "        samples = self.sample_replays(batchsize)\n",
    "        sampled_states = []\n",
    "        sampled_targets = []\n",
    "        for sample in samples:\n",
    "            state, action, reward, new_state, done = sample\n",
    "            input_state = np.append(state, [[action]], axis=1)\n",
    "            target = new_state\n",
    "            sampled_states.append(input_state)\n",
    "            sampled_targets.append(target)\n",
    "\n",
    "        batched_inputs = np.concatenate(sampled_states, axis=0)\n",
    "        batched_targets = np.concatenate(sampled_targets, axis=0)\n",
    "        self.dynamics_model.fit(batched_inputs, batched_targets, epochs=1, verbose=0)\n",
    "\n",
    "    #debug use only\n",
    "    def eval_dynamics_model(self):\n",
    "        samples = self.sample_replays(32)\n",
    "        sampled_states = []\n",
    "        sampled_targets = []\n",
    "        for sample in samples:\n",
    "            state, action, reward, new_state, done = sample\n",
    "            input_state = np.append(state, [[action]], axis=1)\n",
    "            target = new_state\n",
    "            sampled_states.append(input_state)\n",
    "            sampled_targets.append(target)\n",
    "\n",
    "        batched_inputs = np.concatenate(sampled_states, axis=0)\n",
    "        batched_targets = np.concatenate(sampled_targets, axis=0)\n",
    "        scores = self.dynamics_model.evaluate(batched_inputs,batched_targets,verbose=0)\n",
    "        if scores < 0.005:\n",
    "            self.dynamics_model_converged = True\n",
    "            print('Dynamics model has converged!')\n",
    "        print(self.dynamics_model.metrics_names, scores)"
   ],
   "id": "8b06ef55af2d822f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "\n",
    "def save_actor_distribution(agent_distribution, path=None):\n",
    "    df = pd.DataFrame.from_dict(agent_distribution, orient='index', columns=['count'])\n",
    "    df.index.name = 'actor'\n",
    "    if path is not None:\n",
    "        df.to_csv(path, mode='a', header=False)\n",
    "\n",
    "def save_rewards_and_length(rewards, path=None):\n",
    "    df = pd.DataFrame(rewards)\n",
    "    df.to_csv(path, mode='a', header=False)"
   ],
   "id": "e2b144158ed1dd1f"
  },
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import gymnasium as gym\n",
    "import ale_py\n",
    "import numpy as np\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "gym.register_envs(ale_py)\n",
    "env = gym.make(env_name)\n",
    "env.reset(seed=seed)\n",
    "# env = wrappers.Monitor(env, 'replay', video_callable=lambda e: e%record_video_every == 0,force=True)\n",
    "\n",
    "\n",
    "state_shape = (1,env.observation_space.shape[0])\n",
    "agents = []\n",
    "\n",
    "def train_agent(agent, env, max_episodes, state_shape):\n",
    "    \"\"\" Train a single agent on the given environment\"\"\"\n",
    "    start_time = time.time()\n",
    "    total_reward_list = []\n",
    "    episode_length_list = []\n",
    "    for episode in range(max_episodes):\n",
    "        agent.on_episode_start()\n",
    "        state, _ = env.reset()\n",
    "        cur_state = state.reshape(state_shape)\n",
    "        steps = 0\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        while not done:\n",
    "            steps += 1\n",
    "            action = agent.act(cur_state)\n",
    "            new_state, reward, done, truncated, _ = env.step(action)\n",
    "            if truncated:\n",
    "                done = True\n",
    "            new_state = new_state.reshape(state_shape)\n",
    "            agent.update_model(cur_state, action, reward, new_state, done)\n",
    "            cur_state = new_state\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        agent.on_episode_end()\n",
    "        total_reward_list.append(total_reward)\n",
    "        episode_length_list.append(steps)\n",
    "        print('episode {} steps: {}, total reward: {},  elapsed time: {}s'.format(episode, steps, total_reward, int(time.time()-start_time)))\n",
    "\n",
    "def ensemble_training(env, method, epsilon, rng):\n",
    "    obs, _ = env.reset(seed=seed)\n",
    "    cumulative_reward = 0.0\n",
    "    agent_distribution = {agent.name: 0 for agent in agents}\n",
    "    cur_state = obs.reshape(state_shape)\n",
    "    terminated = False\n",
    "    while not terminated:\n",
    "        action, actor_index = method.act(obs)\n",
    "        if rng.rand() < epsilon:\n",
    "            action = rng.randint(env.action_space.n)\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        if truncated:\n",
    "            terminated = True\n",
    "        agent_distribution[agents[actor_index].name] += 1\n",
    "        clipped_reward = np.clip(reward, -1.0, 1.0)\n",
    "        new_state = obs.reshape(state_shape)\n",
    "        agents[actor_index].update_model(cur_state, action, reward, new_state, terminated)\n",
    "\n",
    "        # Update both agents\n",
    "        # agents[0].update_model(cur_state, action, reward, new_state, terminated)\n",
    "        # agents[1].update_model(cur_state, action, reward, new_state, terminated)\n",
    "\n",
    "        cur_state = new_state\n",
    "        method.observe(action, obs, clipped_reward, terminated)\n",
    "        cumulative_reward += reward\n",
    "\n",
    "    save_actor_distribution(agent_distribution, f'actor_distribution_{seed}.csv')\n",
    "    return cumulative_reward\n",
    "\n",
    "\n",
    "def pixel_to_float(obs):\n",
    "    return np.array(obs, dtype=np.float32) / 255.0\n",
    "\n",
    "def atari_evaluation(env, method, epsilon, rng):\n",
    "    pass\n",
    "\n",
    "\n",
    "if dqn_path is not None:\n",
    "    with open(dqn_path, 'rb') as f:\n",
    "        agents.append(pickle.load(f))\n",
    "else:\n",
    "    agents.append(DQN_Agent(env=env, name='DQN_Agent'))\n",
    "    train_agent(agents[0], env, initial_training, state_shape)\n",
    "\n",
    "if model_dqn_path is not None:\n",
    "    with open(model_dqn_path, 'rb') as f:\n",
    "        agents.append(pickle.load(f))\n",
    "else:\n",
    "    agents.append(DQN_Guided_Exploration(env=env, name='DQN_Guided_Exploration'))\n",
    "    train_agent(agents[1], env, initial_training, state_shape)\n",
    "\n",
    "method = TDWVoteEnsemble(agents)\n",
    "\n",
    "# train with ensemble\n",
    "for i in range(max_evaluations):\n",
    "    if env_name.startswith(\"ALE/\"):\n",
    "        reward = atari_evaluation(env, method, epsilon=0.05, rng=np.random.RandomState(0))\n",
    "    else:\n",
    "        reward = ensemble_training(env, method, epsilon=0.05, rng=np.random.RandomState(0))\n",
    "    save_rewards_and_length([reward], f'tdw_rewards_{seed}.csv')"
   ],
   "id": "initial_id"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
